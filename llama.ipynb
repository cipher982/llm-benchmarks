{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/drose/miniconda3/envs/dolly2/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/drose/miniconda3/envs/dolly2/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/drose/miniconda3/envs/dolly2/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d319ddf82e14626b632ab136e860ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "/home/drose/miniconda3/envs/dolly2/lib/python3.9/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Model: decapoda-research/llama-7b-hf =====\n",
      "Output tokens: 1024\n",
      "GPU memory usage: 6.69 GB\n",
      "Time: 59.75 s\n",
      "Tokens per second: 17.21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611824eaa4ac4909bf3e70b1b6074a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Model: decapoda-research/llama-13b-hf =====\n",
      "Output tokens: 1024\n",
      "GPU memory usage: 12.59 GB\n",
      "Time: 72.93 s\n",
      "Tokens per second: 14.11\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import gc\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "\n",
    "models = [\"decapoda-research/llama-7b-hf\", \"decapoda-research/llama-13b-hf\"]\n",
    "\n",
    "for model_name in models:\n",
    "    # Load Model\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model.eval()\n",
    "    model = torch.compile(model)\n",
    "\n",
    "    # Tokenize inputs\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    text = \"Question: Tell me a history of WW2 in 3 or 4 paragraphs.\\nAnswer: \"\n",
    "    input_tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "    # Generate\n",
    "    time0 = time()\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "        input_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        max_length=1024,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    time1 = time()\n",
    "\n",
    "    # Collect metrics\n",
    "    gpu_mem_usage = torch.cuda.memory_allocated() / 1024**3\n",
    "    output_tokens = output.cpu().numpy().tolist()[0]\n",
    "\n",
    "    # Clear up memory\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"===== Model: {model_name} =====\")\n",
    "    print(f\"Output tokens: {len(output_tokens)}\")\n",
    "    print(f\"GPU memory usage: {gpu_mem_usage:.2f} GB\")\n",
    "    print(f\"Time: {time() - time0:.2f} s\")\n",
    "    print(f\"Tokens per second: {len(output_tokens) / (time1 - time0):.2f}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolly2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
